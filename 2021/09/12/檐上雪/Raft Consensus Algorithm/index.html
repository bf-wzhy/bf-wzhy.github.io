<!DOCTYPE html>
<html lang="en">

<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8">
	<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
	
	<!-- title -->
	
	<title>
	
		Raft Consensus Algorithm | 
	 
	Wzhy&#39;s Blog
	</title>
	
	<!-- keywords,description -->
	 

	<!-- favicon -->
	
	<link rel="shortcut icon" href="/favicon.ico">
	


	<!-- search -->
	<script>
		var searchEngine = "https://www.google.com/search?q=";
		if(typeof searchEngine == "undefined" || searchEngine == null || searchEngine == ""){
			searchEngine = "https://www.google.com/search?q=";
		}
		var homeHost = "bf-wzhy.github.io";
		if(typeof homeHost == "undefined" || homeHost == null || homeHost == ""){
			homeHost = window.location.host;
		}
	</script>


	
<link rel="stylesheet" href="/css/main.css">

	
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/styles/darcula.min.css">

	
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">


	
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/highlight.min.js"></script>

	
<script src="https://cdn.jsdelivr.net/npm/jquery-pjax@2.0.1/jquery.pjax.min.js"></script>

	
<script src="/js/main.js"></script>

	
		
<script src="https://cdn.jsdelivr.net/npm/leancloud-storage/dist/av-min.js"></script>

		
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>

	
	
		<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
	<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?3efe99c287df5a1d6f0d02d187e403c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

<header id="header">
    <a id="title" href="/" class="logo">Wzhy's Blog</a>

	<ul id="menu">
		<li class="menu-item">
			<a href="/about" class="menu-item-link">ABOUT</a>
		</li>
	
		<li class="menu-item">
			<a href="/tags" class="menu-item-link">标签</a>
		</li>
	

	
		<li class="menu-item">
			<a href="/categories" class="menu-item-link">分类</a>
		</li>
	

		<li class="menu-item">
			<a href="https://github.com/bf-wzhy" class="menu-item-link" target="_blank">
				<i class="fa fa-github fa-2x"></i>
			</a>
		</li>
	</ul>
</header>

	
<div id="sidebar">
	<button id="sidebar-toggle" class="toggle" ><i class="fa fa-arrow-right " aria-hidden="true"></i></button>
	
	<div id="site-toc">
		<input id="search-input" class="search-input" type="search" placeholder="按回车全站搜索">
		<div id="tree">
			

			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										檐上雪
									</a>
									
							<ul>
								<li class="file active">
									<a href="/2021/09/12/%E6%AA%90%E4%B8%8A%E9%9B%AA/Raft%20Consensus%20Algorithm/">
										Raft Consensus Algorithm
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
							<ul>
								<li class="directory">
									<a href="#" class="directory">
										<i class="fa fa-plus-square-o"></i>
										此间事
									</a>
									
							<ul>
								<li class="file">
									<a href="/2021/09/12/%E6%AD%A4%E9%97%B4%E4%BA%8B/Netty%20Basic%20Tutorial/">
										Netty Basic Tutorial
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
							<ul>
								<li class="file">
									<a href="/2021/09/11/%E6%AD%A4%E9%97%B4%E4%BA%8B/Virtual%20Memory/">
										Virtual Memory
									</a>
								</li>
								<div class="article-toc" style="display: none;"></div>
							</ul>
			
								</li>
								
							</ul>
			
		</div>
	</div>
</div>

	<!-- 引入正文 -->
	<div id="content">
		<h1 id="article-title">

	Raft Consensus Algorithm
</h1>
<div class="article-meta">
	
	<span>Wzhy</span>
	<span>2021-09-12 20:38:54</span>
		<div id="article-categories">
    
		<span>Categories：</span>
            
                
                    <span>
                        <i class="fa fa-folder" aria-hidden="true">
                        <a href="/categories/algorithm/">algorithm</a>
                        </i>
                      
                    </span>
                
            
    

    
		<span>Tags：</span>
            
                
                    <span>
                        <i class="fa fa-tag" aria-hidden="true">
                        <a href="/tags/consensus/">consensus</a>
                        </i>
                    </span>
                
            
    
		</div>

</div>

<div id="article-content">
	<h2 id="Raft，一种易于理解的共识算法"><a href="#Raft，一种易于理解的共识算法" class="headerlink" title="Raft，一种易于理解的共识算法"></a>Raft，一种易于理解的共识算法</h2><p>源论文：In Search of an Understandable Consensus Algorithm</p>
<p>作者：Diego Ongaro and John Ousterhout (Stanford University)</p>
<p>译者：Wzhy</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>​        Raft是一种用于管理replicated log的共识算法。它与(multi-)Paxos在最终结果上是等价的，并且它比Paxos更加高效，但它拥有比Paxos更加易于理解的结构，同时它为构建生产系统提供了更好的基础。为了提高可理解性，Raft分离了共识的关键元素，比如领导者选举，日志复制，安全性，以及密集型的一致性。结果表明Raft是易于学习的。Raft还包括了一种改变集群成员的新机制，它基于重叠的集群主成分来保证安全性。</p>
<h3 id="Replicated状态机"><a href="#Replicated状态机" class="headerlink" title="Replicated状态机"></a>Replicated状态机</h3><p>​        共识算法通常会使用一种称为Replicated状态机的方法。在这种方法中， 集群的状态机计算出一个状态的若干个相同副本。状态机能够在某些服务器down掉后继续执行操作。Replicated状态机被用来解决分布式系统的容错问题。</p>
<p>![image-20210912210834566](Raft Consensus Algorithm.assets/image-20210912210834566.png)</p>
<p>​        Replicated状态机通常基于replicated log实现。每个服务器存储一个包含一系列命令的日志，状态机将会依次执行日志中的每个命令。不同服务器上日志中的命令总是具有相同的顺序，所以状态机在每个服务器上的执行结果是相同的。</p>
<p>​    Keeping the replicated log consistent is the job of the consensus algorithm. The consensus module on a server receives commands from clients and adds them to its log. It communicates with the consensus modules on other servers to ensure that every log eventually contains the same requests in the same order, even if some servers fail. Once commands are properly replicated, each server’s state machine processes them in log order, and the outputs are returned to clients. As a result, the servers appear to form a single, highly reliable state machine. </p>
<p>​    Consensus algorithms for practical systems typically have the following properties: </p>
<ul>
<li>They ensure safety (never returning an incorrect result) under all non-Byzantine conditions, including network delays, partitions, and packet loss, duplication, and reordering. </li>
<li>They are fully functional (available) as long as any majority of the servers are operational and can communicate with each other and with clients. Thus, a typical cluster of five servers can tolerate the failure of any two servers. Servers are assumed to fail by stopping; they may later recover from state on stable storage and rejoin the cluster. </li>
<li>They do not depend on timing to ensure the consistency of the logs: faulty clocks and extreme message delays can, at worst, cause availability problems. </li>
<li>In the common case, a command can complete as soon as a majority of the cluster has responded to a single round of remote procedure calls; a minority of slow servers need not impact overall system performance</li>
</ul>
<h3 id="Designing-for-understandability"><a href="#Designing-for-understandability" class="headerlink" title="Designing for understandability"></a>Designing for understandability</h3><p>​    Two techniques are generally applicable. The first technique is the well-known approach of <strong>problem decomposition</strong>: wherever possible, we divided problems into separate pieces that could be solved, explained, and understood relatively independently. For example, in Raft we separated leader election, log replication, safety, and membership changes. </p>
<p>​    Our second approach was to <strong>simplify the state space</strong> by reducing the number of states to consider, making the system more coherent and eliminating nondeterminism where possible. Specifically, logs are not allowed to have holes, and Raft limits the ways in which logs can become inconsistent with each other. Although in most cases we tried to eliminate nondeterminism, there are some situations where nondeterminism actually improves understandability. In particular, randomized approaches introduce nondeterminism, but they tend to reduce the state space by handling all possible choices in a similar fashion (“choose any; it doesn’t matter”). We used <strong>randomization</strong> to simplify the Raft leader election algorithm.</p>
<h3 id="The-Raft-consensus-algorithm"><a href="#The-Raft-consensus-algorithm" class="headerlink" title="The Raft consensus algorithm"></a>The Raft consensus algorithm</h3><p>​    Raft implements consensus by first electing a distinguished leader, then giving the leader complete responsibility for managing the replicated log. The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines. Having a leader simplifies the management of the replicated log. For example, the leader can decide where to place new entries in the log without consulting other servers, and data flows in a simple fashion from the leader to other servers. A leader can fail or become disconnected from the other servers, in which case a new leader is elected. </p>
<p>​    Given the leader approach, Raft decomposes the consensus problem into three relatively independent subproblems: </p>
<ul>
<li>Leader election: a new leader must be chosen when an existing leader fails. </li>
<li>Log replication: the leader must accept log entries from clients and replicate them across the cluster, forcing the other logs to agree with its own. </li>
<li>Safety: the key safety property for Raft is the State Machine Safety Property in Figure 3: if any server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log index. </li>
</ul>
<p>![image-20210912213243611](Raft Consensus Algorithm.assets/image-20210912213243611.png)</p>
<h4 id="Raft-basics"><a href="#Raft-basics" class="headerlink" title="Raft basics"></a>Raft basics</h4><p>​    A Raft cluster contains several servers; five is a typical number, which allows the system to tolerate(遭受) two failures. At any given time each server is in one of three states: <strong>leader</strong>, <strong>follower</strong>, or <strong>candidate</strong>(候选人). In normal operation there is exactly one leader and all of the other servers are followers. Followers are passive(被动): they issue no requests on their own but simply respond to requests from leaders and candidates. The leader handles all client requests (if a client contacts a follower, the follower redirects it to the leader). The third state, candidate, is used to elect a new leader. </p>
<p>​    Raft divides time into <strong>terms</strong> of arbitrary(任意的) length. Terms are numbered with consecutive(连续的) integers. Each term begins with an election, in which one or more candidates attempt to become leader. If a candidate wins the election, then it serves as leader for the rest of the term. In some situations an election will result in a split vote. In this case the term will end with no leader; a new term (with a new election) will begin shortly. Raft ensures that there is at most one leader in a given term.     </p>
<p>​    Different servers may observe the transitions(转换) between terms at different times, and in some situations a server may not observe an election or even entire terms. Terms act as a <strong>logical clock</strong> in Raft, and they allow servers to detect obsolete(过时的) information such as stale(旧的) leaders. Each server stores a current term number, which increases monotonically(单调地) over time. Current terms are exchanged whenever servers communicate; if one server’s current term is smaller than the other’s, then it updates its current term to the larger value. If a candidate or leader discovers that its term is out of date, it immediately reverts(恢复) to follower state. If a server receives a request with a stale term number, it rejects the request. Raft servers communicate using remote procedure calls (RPCs), and the basic consensus algorithm requires only two types of RPCs. <strong>RequestVote RPCs</strong> are initiated by candidates during elections, and <strong>AppendEntries RPCs</strong> are initiated by leaders to replicate log entries and to provide a form of heartbeat. Servers retry RPCs if they do not receive a response in a timely manner, and they issue RPCs in parallel for best performance.</p>
<h4 id="Leader-election"><a href="#Leader-election" class="headerlink" title="Leader election"></a>Leader election</h4><p>​    Leader election Raft uses a <strong>heartbeat mechanism</strong>(心跳机制) to trigger leader election. When servers start up, they begin as followers. A server remains in follower state as long as it receives valid 5 RPCs from a leader or candidate. Leaders send periodic(周期性的) heartbeats (AppendEntries RPCs that carry no log entries) to all followers in order to maintain their authority(权威性). If a follower receives no communication over a period of time called the <strong>election timeout</strong>(选举超时), then it assumes there is no viable leader and begins an election to choose a new leader.</p>
<p>​    To begin an election, a follower increments its current term and transitions to candidate state. It then votes for itself and issues RequestVote RPCs in parallel to each of the other servers in the cluster. A candidate continues in this state until one of three things happens: (a) it wins the election, (b) another server establishes itself as leader, or (c) a period of time goes by with no winner. </p>
<p>​    A candidate wins an election if it receives votes from a majority of the servers in the full cluster for the same term. <strong>Each server will vote for at most one candidate in a given term</strong>. The majority rule ensures that at most one candidate can win the election for a particular term. Once a candidate wins an election, it becomes leader. It then sends heartbeat messages to all of the other servers to establish its authority and prevent new elections.</p>
<p>​    While waiting for votes, a candidate may receive an AppendEntries RPC from another server claiming(声称) to be leader. If the leader’s term (included in its RPC) is at least as large as the candidate’s current term, then the candidate recognizes the leader as legitimate(合法的) and returns to follower state. If the term in the RPC is smaller than the candidate’s current term, then the candidate rejects the RPC and continues in candidate state.</p>
<p>​    The third possible outcome is that a candidate neither wins nor loses the election: <strong>if many followers become candidates at the same time, votes could be split so that no candidate obtains a majority.</strong> When this happens, each candidate will time out and start a new election by incrementing its term and initiating another round of RequestVote RPCs. However, without extra measures <strong>split votes</strong> could repeat indefinitely.</p>
<p>​    <strong>Raft uses randomized election timeouts to ensure that split votes are rare and that they are resolved quickly.</strong> To prevent split votes in the first place, election timeouts are chosen randomly from a fixed interval (e.g., 150–300ms). This spreads out the servers so that in most cases only a single server will time out; it wins the election and sends heartbeats before any other servers time out. The same mechanism is used to handle split votes. Each candidate restarts its randomized election timeout at the start of an election, and it waits for that timeout to elapse(推移) before starting the next election; this reduces the likelihood(可能性) of another split vote in the new election.</p>
<h4 id="Log-replication"><a href="#Log-replication" class="headerlink" title="Log replication"></a>Log replication</h4><p>​    Once a leader has been elected, it begins servicing client requests. Each <strong>client request contains a command</strong> to be executed by the replicated state machines. The leader appends the command to its log as a new entry, then issues AppendEntries RPCs in parallel to each of the other servers to replicate the entry. When the entry has been safely replicated, the leader applies the entry to its state machine and returns the result of that execution to the client. If followers crash or run slowly, or if network packets are lost, the leader retries AppendEntries RPCs indefinitely (even after it has responded to the client) until all followers eventually store all log entries</p>
<p>​    <strong>Each log entry stores a state machine command</strong> along with the term number when the entry was received by the leader. The term numbers in log entries are used to detect inconsistencies between logs. Each log entry also has an integer index identifying its position in the log.</p>
<p>​    The leader decides when it is safe to apply a log entry to the state machines; such an entry is called committed. Raft <strong>guarantees that committed entries are durable</strong>(持久化的) and will <strong>eventually be executed</strong> by all of the available state machines. A log entry is committed once the leader that created the entry has replicated it on a majority of the servers. This also commits all preceding(之前的) entries in the leader’s log, including entries created by previous leaders. The leader keeps track of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs (including heartbeats) so that the other servers eventually find out. Once a follower learns that a log entry is committed, it applies the entry to its local state machine (in log order).</p>
<p>​    We designed the Raft log mechanism to maintain a high level of coherency(一致性) between the logs on different servers. Not only does this simplify the system’s behavior and make it more predictable, but it is an important component of ensuring safety. Raft maintains the following properties: </p>
<ul>
<li>If two entries in different logs have the same index and term, then they store the same command. </li>
<li>If two entries in different logs have the same index and term, then the logs are identical in all preceding entries.</li>
</ul>
<p>​    <strong>The first property follows from the fact that a leader creates at most one entry with a given log index in a given term, and log entries never change their position in the log. The second property is guaranteed by a simple consistency check performed by AppendEntries.</strong> When sending an AppendEntries RPC, the leader includes the index and term of the entry in its log that immediately precedes the new entries. If the follower does not find an entry in its log with the same index and term, then it refuses the new entries. The consistency check acts as an induction step: the initial empty state of the logs satisfies the Log Matching Property, and the consistency check preserves(保留) the Log Matching Property whenever logs are extended. As a result, whenever AppendEntries returns successfully, the leader knows that the follower’s log is identical to its own log up through the new entries.</p>
<p>​    During normal operation, the logs of the leader and followers stay consistent, so the AppendEntries consistency check never fails. However, leader crashes can leave the logs inconsistent (the old leader may not have fully replicated all of the entries in its log). These inconsistencies can compound(复合) over a series of leader and follower crashes. A follower may be missing entries that are present on the leader, it may have extra entries that are not present on the leader, or both. Missing and extraneous entries in a log may span(跨越) multiple terms.</p>
<p>​    In Raft, <strong>the leader handles inconsistencies by forcing the followers’ logs to duplicate its own.</strong> This means that conflicting entries in follower logs will be overwritten with entries from the leader’s log. </p>
<p>​    <strong>To bring a follower’s log into consistency with its own</strong>, the leader must find the latest log entry where the two logs agree, delete any entries in the follower’s log after that point, and send the follower all of the leader’s entries after that point. All of these actions happen in response to the consistency check performed by AppendEntries RPCs. The leader maintains a nextIndex for each follower, which is the index of the next log entry the leader will send to that follower. When a leader first comes to power, it initializes all nextIndex values to the index just after the last one in its log. If a follower’s log is inconsistent with the leader’s, the AppendEntries consistency check will fail in the next AppendEntries RPC. After a rejection, the leader decrements nextIndex and retries the AppendEntries RPC. Eventually nextIndex will reach a point where the leader and follower logs match. When this happens, AppendEntries will succeed, which removes any conflicting entries in the follower’s log and appends entries from the leader’s log (if any). Once AppendEntries succeeds, the follower’s log is consistent with the leader’s, and it will remain that way for the rest of the term.</p>
<p>​    <strong>If desired, the protocol can be optimized to reduce the number of rejected AppendEntries RPCs.</strong> For example, when rejecting an AppendEntries request, the follower can include the term of the conflicting entry and the first index it stores for that term. With this information, the leader can decrement nextIndex to bypass all of the conflicting entries in that term; one AppendEntries RPC will be required for each term with conflicting entries, rather than one RPC per entry. In practice, we doubt this optimization is necessary, since failures happen infrequently and it is unlikely that there will be many inconsistent entries.</p>
<p>​    With this mechanism, a leader does not need to take any special actions to restore log consistency when it comes to power. It just begins normal operation, and the logs automatically converge(合流) in response to failures of the AppendEntries consistency check. A leader never overwrites or deletes entries in its own log. </p>
<p>​    This log replication mechanism exhibits(展现出) the desirable(理想的) consensus properties: Raft can accept, replicate, and apply new log entries as long as a majority of the servers are up; in the normal case a new entry can be replicated with a single round of RPCs to a majority of the cluster; and a single slow follower will not impact performance.</p>
<h4 id="Safety"><a href="#Safety" class="headerlink" title="Safety"></a>Safety</h4><p>The previous sections described how Raft elects leaders and replicates log entries. However, the mechanisms described so far are not quite sufficient(足够) to ensure that each state machine executes exactly the same commands in the same order. For example, a follower might be unavailable while the leader commits several log entries, then it could be elected leader and overwrite these entries with new ones; as a result, different state machines might execute different command sequences. This section completes the Raft algorithm by adding a restriction(限制) on which servers may be elected leader. The restriction ensures that the leader for any given term contains all of the entries committed in previous terms. Given the election restriction, we then make the rules for commitment more precise(准确). Finally, we present a proof sketch(勾勒) for the Leader Completeness Property and show how it leads to correct behavior of the replicated state machine.</p>
<h5 id="Election-restriction"><a href="#Election-restriction" class="headerlink" title="Election restriction"></a>Election restriction</h5><p>​    In any leader-based consensus algorithm, the leader must eventually store all of the committed log entries. In some consensus algorithms, such as Viewstamped Replication, a leader can be elected even if it doesn’t initially contain all of the committed entries. These algorithms contain additional mechanisms to identify the missing entries and transmit them to the new leader, either during the election process or shortly afterwards. Unfortunately, this results in considerable additional mechanism and complexity. Raft uses a simpler approach where it guarantees that all the committed entries from previous terms are present on each new leader from the moment of its election, without the need to transfer(传输) those entries to the leader. This means that log entries only flow in one direction, from leaders to followers, and leaders never overwrite existing entries in their logs.</p>
<p>​    Raft uses the voting process to prevent a candidate from winning an election unless its log contains all committed entries. A candidate must contact a majority of the cluster in order to be elected, which means that every committed entry must be present in at least one of those servers. If the candidate’s log is at least as up-to-date as any other log in that majority (where “up-to-date” is defined precisely below), then it will hold all the committed entries. The RequestVote RPC implements this restriction: the RPC includes information about the candidate’s log, and the voter denies its vote if its own log is more up-to-date than that of the candidate.</p>
<p>​    <strong>Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs.</strong> If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date.</p>
<h5 id="Committing-entries-from-previous-terms"><a href="#Committing-entries-from-previous-terms" class="headerlink" title="Committing entries from previous terms"></a>Committing entries from previous terms</h5><p>​    A leader knows that an entry from its current term is committed once that entry is stored on a majority of the servers. If a leader crashes before committing an entry, future leaders will attempt to finish replicating the entry. However, a leader cannot immediately conclude that an entry from a previous term is committed once it is stored on a majority of servers. </p>
<p>​    Raft never commits log entries from previous terms by counting replicas. Only log entries from the leader’s current term are committed by counting replicas; once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the Log Matching Property. There are some situations where a leader could safely conclude that an older log entry is committed (for example, if that entry is stored on every server), but Raft takes a more conservative(保守的) approach for simplicity.</p>
<p>​    Raft incurs(招致) this extra complexity in the commitment rules because log entries retain their original term numbers when a leader replicates entries from previous terms. In other consensus algorithms, if a new leader rereplicates entries from prior “terms,” it must do so with its new “term number.” Raft’s approach makes it easier to reason about log entries, since they maintain the same term number over time and across logs. In addition, new leaders in Raft send fewer log entries from previous terms than in other algorithms (other algorithms must send redundant log entries to renumber them before they can be committed).</p>
<h5 id="Safety-argument"><a href="#Safety-argument" class="headerlink" title="Safety argument"></a>Safety argument</h5><p>​    Given the complete Raft algorithm, we can now argue more precisely that the Leader Completeness Property holds (this argument is based on the safety proof). We assume that the Leader Completeness Property does not hold, then we prove a contradiction(矛盾). Suppose the leader for term T (leaderT) commits a log entry from its term, but that log entry is not stored by the leader of some future term. Consider the smallest term U &gt; T whose leader (leaderU) does not store the entry.</p>
<ol>
<li>The committed entry must have been absent from leaderU’s log at the time of its election (leaders never delete or overwrite entries). </li>
<li>leaderT replicated the entry on a majority of the cluster, and leaderU received votes from a majority of the cluster. Thus, at least one server (“the voter”) both accepted the entry from leaderT and voted for leaderU. The voter is key to reaching a contradiction. </li>
<li>The voter must have accepted the committed entry from leaderT before voting for leaderU; otherwise it would have rejected the AppendEntries request from leaderT (its current term would have been higher than T). </li>
<li>The voter still stored the entry when it voted for leaderU, since every intervening leader contained the entry (by assumption), leaders never remove entries, and followers only remove entries if they conflict with the leader. </li>
<li>The voter granted its vote to leaderU, so leaderU’s log must have been as up-to-date as the voter’s. This leads to one of two contradictions. </li>
<li>First, if the voter and leaderU shared the same last log term, then leaderU’s log must have been at least as long as the voter’s, so its log contained every entry in the voter’s log. This is a contradiction, since the voter contained the committed entry and leaderU was assumed not to. </li>
<li>Otherwise, leaderU’s last log term must have been larger than the voter’s. Moreover, it was larger than T, since the voter’s last log term was at least T (it contains the committed entry from term T). The earlier leader that created leaderU’s last log entry must have contained the committed entry in its log (by assumption). Then, by the Log Matching Property, leaderU’s log must also contain the committed entry, which is a contradiction.</li>
<li>This completes the contradiction. Thus, the leaders of all terms greater than T must contain all entries from term T that are committed in term T. </li>
<li>The Log Matching Property guarantees that future leaders will also contain entries that are committed indirectly.</li>
</ol>
<p>​    Given the Leader Completeness Property, we can prove the State Machine Safety Property, which states that if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index. At the time a server applies a log entry to its state machine, its log must be identical to the leader’s log up through that entry and the entry must be committed. Now consider the lowest term in which any server applies a given log index; the Log Completeness Property guarantees that the leaders for all higher terms will store that same log entry, so servers that apply the index in later terms will apply the same value. Thus, the State Machine Safety Property holds.        </p>
<p>​    Finally, Raft requires servers to apply entries in log index order. Combined with the State Machine Safety Property, this means that all servers will apply exactly the same set of log entries to their state machines, in the same order.</p>
<h4 id="Follower-and-candidate-crashes"><a href="#Follower-and-candidate-crashes" class="headerlink" title="Follower and candidate crashes"></a>Follower and candidate crashes</h4><p>​    Until this point we have focused on leader failures. Follower and candidate crashes are much simpler to handle than leader crashes, and they are both handled in the same way. If a follower or candidate crashes, then future RequestVote and AppendEntries RPCs sent to it will fail. Raft handles these failures by retrying indefinitely(无限期地); if the crashed server restarts, then the RPC will complete successfully. If a server crashes after completing an RPC but before responding, then it will receive the same RPC again after it restarts. Raft RPCs are idempotent(幂等的), so this causes no harm. For example, if a follower receives an AppendEntries request that includes log entries already present in its log, it ignores those entries in the new request.</p>
<h4 id="Timing-and-availability"><a href="#Timing-and-availability" class="headerlink" title="Timing and availability"></a>Timing and availability</h4><p>​    One of our requirements for Raft is that safety must not depend on timing: the system must not produce incorrect results just because some event happens more quickly or slowly than expected. However, availability (the ability of the system to respond to clients in a timely manner) must inevitably(不可避免地) depend on timing. For example, if message exchanges take longer than the typical time between server crashes, candidates will not stay up long enough to win an election; without a steady leader, Raft cannot make progress.</p>
<p>​    Leader election is the aspect of Raft where timing is most critical(关键的). Raft will be able to elect and maintain a steady leader as long as the system satisfies the following timing requirement:<br>$$<br>broadcastTime \leq  electionTimeout \leq MTBF<br>$$<br>​    In this inequality broadcastTime is the average time it takes a server to send RPCs in parallel to every server in the cluster and receive their responses; and MTBF is the average time between failures for a single server. The broadcast time should be an order of magnitude(大小) less than the election timeout so that leaders can reliably send the heartbeat messages required to keep followers from starting elections; given the randomized approach used for election timeouts, this inequality also makes split votes unlikely. The election timeout should be a few orders of magnitude less than MTBF so that the system makes steady progress. When the leader crashes, the system will be unavailable for roughly the election timeout; we would like this to represent only a small fraction of overall time.</p>
<p>​    The broadcast time and MTBF are properties of the underlying system, while the election timeout is something we must choose. Raft’s RPCs typically require the recipient(接收者) to persist information to stable storage, so the broadcast time may range from 0.5ms to 20ms, depending on storage technology. As a result, the election timeout is likely to be somewhere between 10ms and 500ms. Typical server MTBFs are several months or more, which easily satisfies the timing requirement.</p>

</div>


    <div class="post-guide">
        <div class="item left">
            
        </div>
        <div class="item right">
            
              <a href="/2021/09/12/%E6%AD%A4%E9%97%B4%E4%BA%8B/Netty%20Basic%20Tutorial/">
                Netty Basic Tutorial
                <i class="fa fa-angle-right" aria-hidden="true"></i>
              </a>
            
        </div>
    </div>



	<div id="vcomments"></div>


<script>
	
		// 评论
		new Valine({
			av: AV,
			el: '#vcomments',
			notify: false,
			verify: false,
			path: window.location.pathname,
			appId: 'lhxsDpFXRxGVfkBtE4GhzLiK-MdYXbMMI',
			appKey: 'PmHF0wDQT0gJEN1J7b8asUi8',
			placeholder: '请输入评论',
			avatar: 'retro',
			recordIP: false
		})
	
	
</script>
	</div>
	<div id="footer">
	<p>
	©2019-<span id="footerYear"></span> 
	<a href="/">Wzhy</a> 
	
	
		|
		<span id="busuanzi_container_site_pv">
			pv
			<span id="busuanzi_value_site_pv"></span>
		</span>
		|
		<span id="busuanzi_container_site_uv"> 
			uv
			<span id="busuanzi_value_site_uv"></span>
		</span>
	
	<br>
	Theme <a href="//github.com/wujun234/hexo-theme-tree" target="_blank">Tree</a>
	by <a href="//github.com/wujun234" target="_blank">WuJun</a>
	Powered by <a href="//hexo.io" target="_blank">Hexo</a>
	</p>
</div>
<script type="text/javascript"> 
	document.getElementById('footerYear').innerHTML = new Date().getFullYear() + '';
</script>
	<button id="totop-toggle" class="toggle"><i class="fa fa-angle-double-up" aria-hidden="true"></i></button>
</body>
</html>